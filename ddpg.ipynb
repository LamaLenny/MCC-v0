{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "from model import Actor\n",
    "from model import Critic\n",
    "from buffer import Buffer\n",
    "from noise import OUStrategy\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "GAMMA = 0.99\n",
    "TAU = 1e-3\n",
    "BUF_SIZE = 4096\n",
    "BATCH_SIZE = 256\n",
    "LR = 1e-4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.target_c = copy.deepcopy(self.critic)\n",
    "\n",
    "        self.actor = Actor(state_dim).to(device)\n",
    "        self.target_a = copy.deepcopy(self.actor)\n",
    "\n",
    "        self.optimizer_c = optim.Adam(self.critic.parameters(), lr=LR)\n",
    "        self.optimizer_a = optim.Adam(self.actor.parameters(), lr=LR)\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(np.array(state)).float().to(device)\n",
    "        return self.actor.forward(state).detach().squeeze(0).cpu().numpy()\n",
    "\n",
    "    def update(self, batch):\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.from_numpy(np.array(states)).float().to(device)\n",
    "        actions = torch.from_numpy(np.array(actions)).float().to(device)\n",
    "        rewards = torch.from_numpy(np.array(rewards)).float().to(device).unsqueeze(1)\n",
    "        next_states = torch.from_numpy(np.array(next_states)).float().to(device)\n",
    "        dones = torch.from_numpy(np.array(dones)).to(device)\n",
    "\n",
    "        Q_current = self.critic(states, actions)\n",
    "        Q_next = self.target_c(next_states, self.target_a(next_states).detach())\n",
    "        y = (rewards + GAMMA * Q_next).detach()\n",
    "\n",
    "        ##################Update critic#######################\n",
    "        loss_c = F.mse_loss(y, Q_current)\n",
    "        self.optimizer_c.zero_grad()\n",
    "        loss_c.backward()\n",
    "        self.optimizer_c.step()\n",
    "\n",
    "        ##################Update actor#######################\n",
    "        loss_a = -self.critic.forward(states, self.actor(states)).mean()\n",
    "        self.optimizer_a.zero_grad()\n",
    "        loss_a.backward()\n",
    "        self.optimizer_a.step()\n",
    "\n",
    "        ##################Update targets#######################\n",
    "        for target_pr, pr in zip(self.target_a.parameters(), self.actor.parameters()):\n",
    "            target_pr.data.copy_(TAU * pr.data + (1 - TAU) * target_pr.data)\n",
    "\n",
    "        for target_pr, pr in zip(self.target_c.parameters(), self.critic.parameters()):\n",
    "            target_pr.data.copy_(TAU * pr.data + (1 - TAU) * target_pr.data)\n",
    "        \n",
    "    def testing(self, num_repeat=100):\n",
    "        env_ = gym.make('MountainCarContinuous-v0')\n",
    "        rews = np.zeros(shape=(num_repeat, ))\n",
    "        for k in range(num_repeat):\n",
    "            state = env_.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.act(state)\n",
    "                state, reward, done, _ = env_.step([action])\n",
    "                rews[k] += reward\n",
    "        return np.mean(rews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I did 0th episode. Result: 85.29541299223042, sigma = 0.298776408\n",
      "test_mean_reward = -0.613522488802396\n",
      "I did 1th episode. Result: 70.23170211316557, sigma = 0.296560147\n",
      "I did 2th episode. Result: 80.06586350641675, sigma = 0.294889704\n",
      "I did 3th episode. Result: 81.10407377311334, sigma = 0.29389403599999997\n",
      "I did 4th episode. Result: 88.75603232836036, sigma = 0.293177275\n",
      "I did 5th episode. Result: 80.88380839077469, sigma = 0.291554816\n",
      "I did 6th episode. Result: 91.04907534163837, sigma = 0.29096101399999996\n",
      "I did 7th episode. Result: -26.53283269950482, sigma = 0.287965013\n",
      "I did 8th episode. Result: 87.6861493942902, sigma = 0.286783407\n",
      "I did 9th episode. Result: 82.83763496669746, sigma = 0.285703767\n",
      "I did 10th episode. Result: 88.04199659479687, sigma = 0.284396203\n",
      "test_mean_reward = -0.061572509417478515\n",
      "I did 11th episode. Result: 93.5441474924562, sigma = 0.283901368\n",
      "I did 12th episode. Result: 86.66400893856255, sigma = 0.28280073499999997\n",
      "I did 13th episode. Result: 87.29295183739603, sigma = 0.281577143\n",
      "I did 14th episode. Result: 85.68384237748994, sigma = 0.28067144499999996\n",
      "I did 15th episode. Result: 89.33762601300356, sigma = 0.280074644\n",
      "I did 16th episode. Result: 90.89440670223226, sigma = 0.279393871\n",
      "I did 17th episode. Result: 87.15320720069113, sigma = 0.278155284\n",
      "I did 18th episode. Result: 90.30473855333601, sigma = 0.277339556\n",
      "I did 19th episode. Result: 89.20480268840478, sigma = 0.276460849\n",
      "I did 20th episode. Result: 89.44133874100483, sigma = 0.275633125\n",
      "test_mean_reward = 14.851188689236844\n",
      "I did 21th episode. Result: 90.02019683266224, sigma = 0.274796404\n",
      "I did 22th episode. Result: 93.76779045647432, sigma = 0.274382542\n",
      "I did 23th episode. Result: 90.62468469074808, sigma = 0.273536824\n",
      "I did 24th episode. Result: 87.312397770498, sigma = 0.27279307199999997\n",
      "I did 25th episode. Result: 91.29340971416985, sigma = 0.27198334199999996\n",
      "I did 26th episode. Result: 90.62691944038649, sigma = 0.270864715\n",
      "I did 27th episode. Result: 91.39177689025792, sigma = 0.270183942\n",
      "I did 28th episode. Result: 89.43679865938843, sigma = 0.269503169\n",
      "I did 29th episode. Result: 86.76745982091965, sigma = 0.268252586\n",
      "I did 30th episode. Result: 86.76048401745356, sigma = 0.267295905\n",
      "test_mean_reward = 29.683484088997282\n",
      "I did 31th episode. Result: 93.61554055895084, sigma = 0.26662113\n",
      "I did 32th episode. Result: 94.88430780997237, sigma = 0.266093306\n",
      "I did 33th episode. Result: 89.46929368955833, sigma = 0.265139624\n",
      "I did 34th episode. Result: 87.86553446378747, sigma = 0.26415295299999997\n",
      "I did 35th episode. Result: 83.77647703214326, sigma = 0.262128628\n",
      "I did 36th episode. Result: 93.67045630233972, sigma = 0.26160680199999997\n",
      "I did 37th episode. Result: 86.3893529907678, sigma = 0.260524163\n",
      "I did 38th episode. Result: 74.4170558271194, sigma = 0.25876374999999996\n",
      "I did 39th episode. Result: 82.86904073357215, sigma = 0.25735122099999996\n",
      "I did 40th episode. Result: 85.14825389695818, sigma = 0.25640653599999996\n",
      "test_mean_reward = -24.732833861840163\n",
      "I did 41th episode. Result: 79.85350156730746, sigma = 0.255122964\n",
      "I did 42th episode. Result: 74.67970247008486, sigma = 0.25360547\n",
      "I did 43th episode. Result: 78.65860438757517, sigma = 0.252387876\n",
      "I did 44th episode. Result: 92.70562480429521, sigma = 0.251983011\n",
      "I did 45th episode. Result: 86.89387897968685, sigma = 0.251083311\n",
      "I did 46th episode. Result: 73.23590730236764, sigma = 0.24907698\n",
      "I did 47th episode. Result: 75.6233819039743, sigma = 0.247436527\n",
      "I did 48th episode. Result: 91.79386658077404, sigma = 0.24698367799999998\n",
      "I did 49th episode. Result: 74.49965844613038, sigma = 0.245727097\n",
      "I did 50th episode. Result: -46.27282993229981, sigma = 0.24273109599999998\n",
      "test_mean_reward = -44.368576675249805\n",
      "I did 51th episode. Result: 81.27591261927343, sigma = 0.241492509\n",
      "I did 52th episode. Result: 76.66045348792473, sigma = 0.240190943\n",
      "I did 53th episode. Result: 88.45185366589641, sigma = 0.239252256\n",
      "I did 54th episode. Result: 74.05660949425265, sigma = 0.23739287599999997\n",
      "I did 55th episode. Result: 94.42127092753017, sigma = 0.23705998699999997\n",
      "I did 56th episode. Result: 88.63139737902476, sigma = 0.23624425899999998\n",
      "I did 57th episode. Result: 94.47853587286527, sigma = 0.23573742799999997\n",
      "I did 58th episode. Result: 86.5988404438351, sigma = 0.23489470899999998\n",
      "I did 59th episode. Result: 93.05227399278499, sigma = 0.234480847\n",
      "I did 60th episode. Result: 93.69541457905926, sigma = 0.233971017\n",
      "test_mean_reward = 92.10696888450744\n",
      "I did 61th episode. Result: 92.18804868492148, sigma = 0.233461187\n",
      "I did 62th episode. Result: 91.83512743657397, sigma = 0.232747425\n",
      "I did 63th episode. Result: 90.47506518302605, sigma = 0.232093643\n",
      "I did 64th episode. Result: 95.03859150425096, sigma = 0.231838728\n",
      "I did 65th episode. Result: 92.08885573771168, sigma = 0.231469851\n",
      "I did 66th episode. Result: 95.1170454955061, sigma = 0.231196942\n",
      "I did 67th episode. Result: 91.61662954660818, sigma = 0.230780081\n",
      "I did 68th episode. Result: 93.13356103197312, sigma = 0.230330231\n",
      "I did 69th episode. Result: 94.29410139517105, sigma = 0.230021334\n"
     ]
    }
   ],
   "source": [
    "episodes = 70\n",
    "\n",
    "seed = 22\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "agent = DDPG(2, 1)\n",
    "buf = Buffer(BUF_SIZE)\n",
    "noise = OUStrategy(env.action_space, min_sigma=1e-4)\n",
    "updates_noise = 0\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        action = noise.get_action_from_raw_action(action, updates_noise)\n",
    "        updates_noise += 1\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        buf.add((state, action, reward, next_state, done))\n",
    "        if len(buf) >= BATCH_SIZE:\n",
    "            agent.update(buf.sample(BATCH_SIZE))\n",
    "        state = next_state\n",
    "    print(f\"I did {episode}th episode. Result: {total_reward}, sigma = {noise.sigma}\")\n",
    "    if not episode % 10:\n",
    "        print(f'test_mean_reward = {agent.testing()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
