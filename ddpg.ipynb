{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "from model import Actor\n",
    "from model import Critic\n",
    "from buffer import Buffer\n",
    "from noise import OUStrategy\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "GAMMA = 0.99\n",
    "TAU = 1e-3\n",
    "BUF_SIZE = 4096\n",
    "BATCH_SIZE = 256\n",
    "LR = 1e-4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.target_c = copy.deepcopy(self.critic)\n",
    "\n",
    "        self.actor = Actor(state_dim).to(device)\n",
    "        self.target_a = copy.deepcopy(self.actor)\n",
    "\n",
    "        self.optimizer_c = optim.Adam(self.critic.parameters(), lr=LR)\n",
    "        self.optimizer_a = optim.Adam(self.actor.parameters(), lr=LR)\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(np.array(state)).float().to(device)\n",
    "        return self.actor.forward(state).detach().squeeze(0).cpu().numpy()\n",
    "\n",
    "    def update(self, batch):\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.from_numpy(np.array(states)).float().to(device)\n",
    "        actions = torch.from_numpy(np.array(actions)).float().to(device)\n",
    "        rewards = torch.from_numpy(np.array(rewards)).float().to(device).unsqueeze(1)\n",
    "        next_states = torch.from_numpy(np.array(next_states)).float().to(device)\n",
    "        dones = torch.from_numpy(np.array(dones)).to(device)\n",
    "\n",
    "        Q_current = self.critic(states, actions)\n",
    "        Q_next = self.target_c(next_states, self.target_a(next_states).detach())\n",
    "        y = (rewards + GAMMA * Q_next).detach()\n",
    "\n",
    "        ##################Update critic#######################\n",
    "        loss_c = F.mse_loss(y, Q_current)\n",
    "        self.optimizer_c.zero_grad()\n",
    "        loss_c.backward()\n",
    "        self.optimizer_c.step()\n",
    "\n",
    "        ##################Update actor#######################\n",
    "        loss_a = -self.critic.forward(states, self.actor(states)).mean()\n",
    "        self.optimizer_a.zero_grad()\n",
    "        loss_a.backward()\n",
    "        self.optimizer_a.step()\n",
    "\n",
    "        ##################Update targets#######################\n",
    "        for target_pr, pr in zip(self.target_a.parameters(), self.actor.parameters()):\n",
    "            target_pr.data.copy_(TAU * pr.data + (1 - TAU) * target_pr.data)\n",
    "\n",
    "        for target_pr, pr in zip(self.target_c.parameters(), self.critic.parameters()):\n",
    "            target_pr.data.copy_(TAU * pr.data + (1 - TAU) * target_pr.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I did 0th episode. Result: 85.29541299223042, sigma = 0.298776408, mean reward = 85.29541299223042\n",
      "I did 1th episode. Result: 70.23170211316557, sigma = 0.296560147, mean reward = 77.76355755269799\n",
      "I did 2th episode. Result: 80.06586350641675, sigma = 0.294889704, mean reward = 78.53099287060424\n",
      "I did 3th episode. Result: 81.10407377311334, sigma = 0.29389403599999997, mean reward = 79.17426309623151\n",
      "I did 4th episode. Result: 88.75603232836036, sigma = 0.293177275, mean reward = 81.09061694265729\n",
      "I did 5th episode. Result: 80.88380839077469, sigma = 0.291554816, mean reward = 81.05614885067685\n",
      "I did 6th episode. Result: 91.04907534163837, sigma = 0.29096101399999996, mean reward = 82.48370977795707\n",
      "I did 7th episode. Result: -26.53283269950482, sigma = 0.287965013, mean reward = 68.85664196827433\n",
      "I did 8th episode. Result: 87.6861493942902, sigma = 0.286783407, mean reward = 70.94880946005387\n",
      "I did 9th episode. Result: 82.83763496669746, sigma = 0.285703767, mean reward = 72.13769201071823\n",
      "I did 10th episode. Result: 88.04199659479687, sigma = 0.284396203, mean reward = 73.5835378819981\n",
      "I did 11th episode. Result: 93.5441474924562, sigma = 0.283901368, mean reward = 75.24692201620294\n",
      "I did 12th episode. Result: 86.66400893856255, sigma = 0.28280073499999997, mean reward = 76.12515947176907\n",
      "I did 13th episode. Result: 87.29295183739603, sigma = 0.281577143, mean reward = 76.92285892645671\n",
      "I did 14th episode. Result: 85.68384237748994, sigma = 0.28067144499999996, mean reward = 77.50692448985893\n",
      "I did 15th episode. Result: 89.33762601300356, sigma = 0.280074644, mean reward = 78.24634333505547\n",
      "I did 16th episode. Result: 90.89440670223226, sigma = 0.279393871, mean reward = 78.99034706253646\n",
      "I did 17th episode. Result: 87.15320720069113, sigma = 0.278155284, mean reward = 79.44383929243395\n",
      "I did 18th episode. Result: 90.30473855333601, sigma = 0.277339556, mean reward = 80.01546556932352\n",
      "I did 19th episode. Result: 89.20480268840478, sigma = 0.276460849, mean reward = 80.47493242527759\n",
      "I did 20th episode. Result: 89.44133874100483, sigma = 0.275633125, mean reward = 80.90190415459793\n",
      "I did 21th episode. Result: 90.02019683266224, sigma = 0.274796404, mean reward = 81.31637200360086\n",
      "I did 22th episode. Result: 93.76779045647432, sigma = 0.274382542, mean reward = 81.857738023291\n",
      "I did 23th episode. Result: 90.62468469074808, sigma = 0.273536824, mean reward = 82.22302746776838\n",
      "I did 24th episode. Result: 87.312397770498, sigma = 0.27279307199999997, mean reward = 82.42660227987757\n",
      "I did 25th episode. Result: 91.29340971416985, sigma = 0.27198334199999996, mean reward = 82.76763333504265\n",
      "I did 26th episode. Result: 90.62691944038649, sigma = 0.270864715, mean reward = 83.05871800561096\n",
      "I did 27th episode. Result: 91.39177689025792, sigma = 0.270183942, mean reward = 83.3563272514912\n",
      "I did 28th episode. Result: 89.43679865938843, sigma = 0.269503169, mean reward = 83.56599867934972\n",
      "I did 29th episode. Result: 86.76745982091965, sigma = 0.268252586, mean reward = 83.67271405073538\n",
      "I did 30th episode. Result: 86.76048401745356, sigma = 0.267295905, mean reward = 83.77231953353275\n",
      "I did 31th episode. Result: 93.61554055895084, sigma = 0.26662113, mean reward = 84.07992019057706\n",
      "I did 32th episode. Result: 94.88430780997237, sigma = 0.266093306, mean reward = 84.40732587601327\n",
      "I did 33th episode. Result: 89.46929368955833, sigma = 0.265139624, mean reward = 84.55620728229401\n",
      "I did 34th episode. Result: 87.86553446378747, sigma = 0.26415295299999997, mean reward = 84.65075948747953\n",
      "I did 35th episode. Result: 83.77647703214326, sigma = 0.262128628, mean reward = 84.6264738637202\n",
      "I did 36th episode. Result: 93.67045630233972, sigma = 0.26160680199999997, mean reward = 84.87090582152072\n",
      "I did 37th episode. Result: 86.3893529907678, sigma = 0.260524163, mean reward = 84.91086495755353\n",
      "I did 38th episode. Result: 74.4170558271194, sigma = 0.25876374999999996, mean reward = 84.64179292856804\n",
      "I did 39th episode. Result: 82.86904073357215, sigma = 0.25735122099999996, mean reward = 84.59747412369316\n",
      "I did 40th episode. Result: 85.14825389695818, sigma = 0.25640653599999996, mean reward = 84.61090777669962\n",
      "I did 41th episode. Result: 79.85350156730746, sigma = 0.255122964, mean reward = 84.49763620028551\n",
      "I did 42th episode. Result: 74.67970247008486, sigma = 0.25360547, mean reward = 84.26931216004829\n",
      "I did 43th episode. Result: 78.65860438757517, sigma = 0.252387876, mean reward = 84.14179607431026\n",
      "I did 44th episode. Result: 92.70562480429521, sigma = 0.251983011, mean reward = 84.33210337942104\n",
      "I did 45th episode. Result: 86.89387897968685, sigma = 0.251083311, mean reward = 84.38779415333985\n",
      "I did 46th episode. Result: 73.23590730236764, sigma = 0.24907698, mean reward = 84.15051996502129\n",
      "I did 47th episode. Result: 75.6233819039743, sigma = 0.247436527, mean reward = 83.97287125541617\n",
      "I did 48th episode. Result: 91.79386658077404, sigma = 0.24698367799999998, mean reward = 84.13248340491326\n",
      "I did 49th episode. Result: 74.49965844613038, sigma = 0.245727097, mean reward = 83.9398269057376\n",
      "I did 50th episode. Result: -46.27282993229981, sigma = 0.24273109599999998, mean reward = 81.38663755597217\n",
      "I did 51th episode. Result: 81.27591261927343, sigma = 0.241492509, mean reward = 81.38450823026642\n",
      "I did 52th episode. Result: 76.66045348792473, sigma = 0.240190943, mean reward = 81.29537512192034\n",
      "I did 53th episode. Result: 88.45185366589641, sigma = 0.239252256, mean reward = 81.42790250236433\n",
      "I did 54th episode. Result: 74.05660949425265, sigma = 0.23739287599999997, mean reward = 81.29387899312594\n",
      "I did 55th episode. Result: 94.42127092753017, sigma = 0.23705998699999997, mean reward = 81.52829670624033\n",
      "I did 56th episode. Result: 88.63139737902476, sigma = 0.23624425899999998, mean reward = 81.65291250751724\n",
      "I did 57th episode. Result: 94.47853587286527, sigma = 0.23573742799999997, mean reward = 81.87404394485083\n",
      "I did 58th episode. Result: 86.5988404438351, sigma = 0.23489470899999998, mean reward = 81.95412524144378\n",
      "I did 59th episode. Result: 93.05227399278499, sigma = 0.234480847, mean reward = 82.13909438729947\n",
      "I did 60th episode. Result: 93.69541457905926, sigma = 0.233971017, mean reward = 82.32854225929553\n",
      "I did 61th episode. Result: 92.18804868492148, sigma = 0.233461187, mean reward = 82.48756655648305\n",
      "I did 62th episode. Result: 91.83512743657397, sigma = 0.232747425, mean reward = 82.6359405387067\n",
      "I did 63th episode. Result: 90.47506518302605, sigma = 0.232093643, mean reward = 82.75842686127419\n",
      "I did 64th episode. Result: 95.03859150425096, sigma = 0.231838728, mean reward = 82.94735247116614\n",
      "I did 65th episode. Result: 92.08885573771168, sigma = 0.231469851, mean reward = 83.08586009641682\n",
      "I did 66th episode. Result: 95.1170454955061, sigma = 0.231196942, mean reward = 83.26543002774652\n",
      "I did 67th episode. Result: 91.61662954660818, sigma = 0.230780081, mean reward = 83.38824178537685\n",
      "I did 68th episode. Result: 93.13356103197312, sigma = 0.230330231, mean reward = 83.52947829619707\n",
      "I did 69th episode. Result: 94.29410139517105, sigma = 0.230021334, mean reward = 83.68325862618242\n",
      "I did 70th episode. Result: 94.16889689687818, sigma = 0.22975442299999999, mean reward = 83.83094367224857\n",
      "I did 71th episode. Result: 94.10961389679885, sigma = 0.2295235, mean reward = 83.97370298092287\n",
      "I did 72th episode. Result: 94.61579731848391, sigma = 0.229049658, mean reward = 84.11948509513603\n",
      "I did 73th episode. Result: 91.93702182968066, sigma = 0.228746759, mean reward = 84.22512748344069\n",
      "I did 74th episode. Result: 94.23586716051351, sigma = 0.228449858, mean reward = 84.35860401246832\n",
      "I did 75th episode. Result: 94.49936319352486, sigma = 0.22819194399999998, mean reward = 84.49203505432433\n",
      "I did 76th episode. Result: 87.82480501132196, sigma = 0.227640128, mean reward = 84.53531778103859\n",
      "I did 77th episode. Result: 90.656033014731, sigma = 0.22701033799999998, mean reward = 84.61378848916284\n",
      "I did 78th episode. Result: 92.03669775873881, sigma = 0.226485513, mean reward = 84.70774936599292\n",
      "I did 79th episode. Result: 93.30685902631762, sigma = 0.225981681, mean reward = 84.81523823674699\n",
      "I did 80th episode. Result: 93.09675830845397, sigma = 0.22569077799999998, mean reward = 84.91747922528658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I did 81th episode. Result: 94.22029680692164, sigma = 0.225435863, mean reward = 85.03092822018458\n",
      "I did 82th episode. Result: 88.80848316447174, sigma = 0.224731098, mean reward = 85.07644093035671\n",
      "I did 83th episode. Result: 87.36167345930845, sigma = 0.22400534, mean reward = 85.1036460795109\n",
      "I did 84th episode. Result: 85.96176065469734, sigma = 0.223159622, mean reward = 85.11374154510133\n",
      "I did 85th episode. Result: 92.91332165249264, sigma = 0.222880715, mean reward = 85.20443433704773\n",
      "I did 86th episode. Result: 95.65993403049043, sigma = 0.222610805, mean reward = 85.32461249444363\n",
      "I did 87th episode. Result: 94.57774545061937, sigma = 0.222313904, mean reward = 85.42976173258198\n",
      "I did 88th episode. Result: 93.01902816179802, sigma = 0.222026, mean reward = 85.51503438909002\n",
      "I did 89th episode. Result: 91.27755201764911, sigma = 0.22155815599999998, mean reward = 85.57906236274069\n",
      "I did 90th episode. Result: 91.49628519236082, sigma = 0.221078316, mean reward = 85.64408678943981\n",
      "I did 91th episode. Result: 90.14453329014015, sigma = 0.220619469, mean reward = 85.69300468618655\n",
      "I did 92th episode. Result: 88.3245862869367, sigma = 0.219968686, mean reward = 85.7213012625387\n",
      "I did 93th episode. Result: 90.4306518551673, sigma = 0.21949184499999996, mean reward = 85.77140073692837\n",
      "I did 94th episode. Result: 94.66775059112436, sigma = 0.219251925, mean reward = 85.86504652486728\n",
      "I did 95th episode. Result: 91.86928494306301, sigma = 0.218772085, mean reward = 85.9275906750568\n",
      "I did 96th episode. Result: 90.98382413165317, sigma = 0.218310239, mean reward = 85.97971679316605\n",
      "I did 97th episode. Result: 90.29877035881916, sigma = 0.21785439099999998, mean reward = 86.02378876832577\n",
      "I did 98th episode. Result: 94.55807899647156, sigma = 0.21761147199999997, mean reward = 86.10999372012522\n",
      "I did 99th episode. Result: 93.74124072876221, sigma = 0.21731757, mean reward = 86.1863061902116\n",
      "I did 100th episode. Result: 89.05040161677236, sigma = 0.216825734, mean reward = 86.22385607645703\n",
      "I did 101th episode. Result: 92.59800075587088, sigma = 0.216519836, mean reward = 86.44751906288407\n",
      "I did 102th episode. Result: 93.58599382479262, sigma = 0.216222935, mean reward = 86.58272036606782\n",
      "I did 103th episode. Result: 89.02368613300258, sigma = 0.21573109899999998, mean reward = 86.66191648966672\n",
      "I did 104th episode. Result: 94.50375187436366, sigma = 0.21543419799999997, mean reward = 86.71939368512676\n",
      "I did 105th episode. Result: 88.51726659325938, sigma = 0.21476842, mean reward = 86.7957282671516\n",
      "I did 106th episode. Result: 92.92562392023193, sigma = 0.21445652399999998, mean reward = 86.81449375293752\n",
      "I did 107th episode. Result: 92.8556969493504, sigma = 0.213970686, mean reward = 88.0083790494261\n",
      "I did 108th episode. Result: 91.36660606887253, sigma = 0.21355382499999997, mean reward = 88.04518361617193\n",
      "I did 109th episode. Result: 88.8459922753316, sigma = 0.212813072, mean reward = 88.10526718925827\n",
      "I did 110th episode. Result: 91.25751058692596, sigma = 0.21231823699999997, mean reward = 88.13742232917956\n",
      "I did 111th episode. Result: 89.85332920354539, sigma = 0.211826401, mean reward = 88.10051414629042\n",
      "I did 112th episode. Result: 91.24636990093649, sigma = 0.211304575, mean reward = 88.14633775591417\n",
      "I did 113th episode. Result: 95.1700705762802, sigma = 0.21103466499999998, mean reward = 88.225108943303\n",
      "I did 114th episode. Result: 87.94338711121937, sigma = 0.21034189599999997, mean reward = 88.24770439064031\n",
      "I did 115th episode. Result: 94.27846981309789, sigma = 0.21005699099999997, mean reward = 88.29711282864127\n",
      "I did 116th episode. Result: 91.15749074049894, sigma = 0.209559157, mean reward = 88.29974366902394\n",
      "I did 117th episode. Result: 91.2577148049879, sigma = 0.209058324, mean reward = 88.34078874506692\n",
      "I did 118th episode. Result: 93.51349369596096, sigma = 0.208779417, mean reward = 88.37287629649316\n",
      "I did 119th episode. Result: 93.62593425556057, sigma = 0.208533499, mean reward = 88.4170876121647\n",
      "I did 120th episode. Result: 89.06097741207083, sigma = 0.208002676, mean reward = 88.41328399887536\n",
      "I did 121th episode. Result: 86.86118175966432, sigma = 0.207147961, mean reward = 88.38169384814539\n",
      "I did 122th episode. Result: 88.90669824605607, sigma = 0.206605142, mean reward = 88.3330829260412\n",
      "I did 123th episode. Result: 92.75223798778924, sigma = 0.20612830099999999, mean reward = 88.35435845901162\n",
      "I did 124th episode. Result: 91.90639619523118, sigma = 0.205780417, mean reward = 88.40029844325895\n",
      "I did 125th episode. Result: 93.84235911764826, sigma = 0.20550750799999998, mean reward = 88.42578793729373\n",
      "I did 126th episode. Result: 92.26517415802242, sigma = 0.20489871099999998, mean reward = 88.4421704844701\n",
      "I did 127th episode. Result: 93.63658477826513, sigma = 0.20442786799999998, mean reward = 88.46461856335016\n",
      "I did 128th episode. Result: 92.78723489772372, sigma = 0.20396602199999997, mean reward = 88.49812292573351\n",
      "I did 129th episode. Result: 88.60827220067726, sigma = 0.203351227, mean reward = 88.5165310495311\n",
      "I did 130th episode. Result: 92.92855804136191, sigma = 0.20288638199999998, mean reward = 88.57821178977018\n",
      "I did 131th episode. Result: 89.372483817787, sigma = 0.20221760499999997, mean reward = 88.53578122235855\n",
      "I did 132th episode. Result: 90.73195584014354, sigma = 0.20165679199999997, mean reward = 88.49425770266025\n",
      "I did 133th episode. Result: 87.66416821006808, sigma = 0.20106299, mean reward = 88.47620644786535\n",
      "I did 134th episode. Result: 94.81543148626493, sigma = 0.200832067, mean reward = 88.54570541809012\n",
      "I did 135th episode. Result: 95.14765834624481, sigma = 0.20060114399999998, mean reward = 88.65941723123113\n",
      "I did 136th episode. Result: 91.86966559185035, sigma = 0.20010031099999998, mean reward = 88.64140932412623\n",
      "I did 137th episode. Result: 90.41423132470626, sigma = 0.19947352, mean reward = 88.68165810746561\n",
      "I did 138th episode. Result: 90.3392128407462, sigma = 0.198909708, mean reward = 88.84087967760189\n",
      "I did 139th episode. Result: 89.44747220034239, sigma = 0.197989015, mean reward = 88.90666399226959\n",
      "I did 140th episode. Result: 92.10991124718097, sigma = 0.19727225399999998, mean reward = 88.97628056577182\n",
      "I did 141th episode. Result: 92.71254832561084, sigma = 0.196807409, mean reward = 89.10487103335485\n",
      "I did 142th episode. Result: 94.85721600050255, sigma = 0.19651950499999998, mean reward = 89.30664616865903\n",
      "I did 143th episode. Result: 91.78523646376388, sigma = 0.196033667, mean reward = 89.43791248942092\n",
      "I did 144th episode. Result: 93.67489055373406, sigma = 0.19557781899999999, mean reward = 89.44760514691532\n",
      "I did 145th episode. Result: 90.93756186344226, sigma = 0.195073987, mean reward = 89.48804197575286\n",
      "I did 146th episode. Result: 91.30436721042396, sigma = 0.19439321399999998, mean reward = 89.66872657483341\n",
      "I did 147th episode. Result: 94.38681673698937, sigma = 0.19399134799999998, mean reward = 89.85636092316355\n",
      "I did 148th episode. Result: 95.2816602032546, sigma = 0.19373643299999999, mean reward = 89.89123885938838\n",
      "I did 149th episode. Result: 91.50565244438486, sigma = 0.193199612, mean reward = 90.0612987993709\n"
     ]
    }
   ],
   "source": [
    "episodes = 150\n",
    "\n",
    "seed = 22\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "agent = DDPG(2, 1)\n",
    "buf = Buffer(BUF_SIZE)\n",
    "noise = OUStrategy(env.action_space, min_sigma=1e-4)\n",
    "updates_noise = 0\n",
    "results = deque(maxlen=100)\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        action = noise.get_action_from_raw_action(action, updates_noise)\n",
    "        updates_noise += 1\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        buf.add((state, action, reward, next_state, done))\n",
    "        if len(buf) >= BATCH_SIZE:\n",
    "            agent.update(buf.sample(BATCH_SIZE))\n",
    "        state = next_state\n",
    "    results.append(total_reward)\n",
    "    print(f\"I did {episode}th episode. Result: {total_reward}, sigma = {noise.sigma}, mean reward = {np.mean(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
